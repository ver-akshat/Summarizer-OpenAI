{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I am using OpenAI API to summarize arxiv research papers using the url of that research paper and reading it from there and making a summary out of that and saving that in a doc file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import PyPDF2  \n",
    "import openai\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method to download PDF file and extract its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_text(pdf_url, output_filepath=None):\n",
    "    response=requests.get(pdf_url,stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with BytesIO(response.content) as pdf_buffer:\n",
    "        pdf_reader=PyPDF2.PdfReader(pdf_buffer)\n",
    "        if pdf_reader.is_encrypted:\n",
    "            raise ValueError(\"Research paper is password-protected. Decrypt or provide password.\")\n",
    "        text=\"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            try:\n",
    "                text+=page.extract_text()\n",
    "            except KeyError:# Handle potential KeyError from PyPDF2\n",
    "                try:\n",
    "                    text+=page.extract_text(\"plain\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting text from page {page.number}: {e}\")\n",
    "        if output_filepath:\n",
    "            with open(output_filepath,\"w\",encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method to use openai to generate summary, since I am using gpt-3-turbo-instruct ,its ontext size if 4096 tokens so if length of text in research paper is greater than 4096, it needs to be truncated otherwise it will give error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with_openai(text, max_tokens=4096):\n",
    "    # Ensure text length fits within API limits\n",
    "    if len(text) > max_tokens:\n",
    "        print(f\"Text exceeds length limit ({max_tokens} tokens). Truncating.\")\n",
    "        text=text[:max_tokens]\n",
    "    prompt=\"Summarize the following research paper:\\n\"+text\n",
    "    response=openai.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=600,  \n",
    "        temperature=0.7,  \n",
    "        n=1,\n",
    "        stop=None,\n",
    "    )\n",
    "    summary=response.choices[0].text.strip()\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running the above 2 functions using arxiv url, one can change value and get summary of any research paper and printing and saving that in a doc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text exceeds length limit (4096 tokens). Truncating.\n",
      "Summary:\n",
      "state-of-the-art sequence-to-sequence models\n",
      "[9, 11, 6]. Most prominently, the Transformer [ 31] architecture combines a self-attention mechanism\n",
      "with a convolutional architecture, achieving state-of-the-art results in machine translation and other\n",
      "sequence tasks. This paper proposes a new model architecture, also based solely on attention\n",
      "mechanisms, dispensing with recurrence and convolutions entirely. In contrast to the existing\n",
      "approaches, our model, the Transformer, adopts a global attentional mechanism, where each output\n",
      "position attends over all input positions. In the Transformer, each input or output position computes\n",
      "an output as a weighted sum of the input positions, where the weight is determined by a compatibility\n",
      "function of the two positions. The compatibility function is a dot product between the input and\n",
      "output positions, divided by a factor that scales the dot product according to the dimensionality of the\n",
      "inputs. This model has several advantages, as it is more parallelizable and requires less time to train, while still achieving superior results in quality. The Transformer also generalizes well to other tasks, as demonstrated in experiments with English constituency parsing.\n"
     ]
    }
   ],
   "source": [
    "# replace with your PDF URL\n",
    "pdf_url=\"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "text=download_and_extract_text(pdf_url)\n",
    "summary=summarize_with_openai(text)\n",
    "print(\"Summary:\")\n",
    "print(\"\\n\")\n",
    "print(summary)\n",
    "print(\"\\n\\n\")\n",
    "print(\"************************************************************************************************\")\n",
    "print(\"Saving to a file\")\n",
    "doc=Document()\n",
    "doc.add_paragraph(summary)\n",
    "doc.save(\"research_paper_summarized.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
